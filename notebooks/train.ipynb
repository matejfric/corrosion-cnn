{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrosion in Industrial Complexes in Ostrava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import albumentations as albu\n",
    "import dagshub\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "from dagshub import get_repo_bucket_client\n",
    "\n",
    "# Note: This does not recover the best weights as in Keras!\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# https://github.com/Lightning-AI/pytorch-lightning/discussions/10399,\n",
    "# https://pytorch-lightning.readthedocs.io/en/1.5.10/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "\n",
    "from corrosion import (\n",
    "    CorrosionModel,\n",
    "    SegmentationDatasetLoader,\n",
    "    SegmentationDatasetSplit,\n",
    ")\n",
    "from corrosion.augmentation import (\n",
    "    compose_transforms,\n",
    "    hard_transforms,\n",
    "    post_transforms,\n",
    "    pre_transforms,\n",
    "    resize_transforms,\n",
    ")\n",
    "from corrosion.git import get_commit_id, get_current_branch\n",
    "from corrosion.plot import (\n",
    "    plot_learning_curves,\n",
    "    plot_predictions,\n",
    "    plot_predictions_compact,\n",
    "    show_examples,\n",
    "    show_random,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Experiment logging\n",
    "REPO_NAME = 'corrosion'\n",
    "USER_NAME = 'matejfric'\n",
    "dagshub.init(REPO_NAME, USER_NAME, mlflow=True)\n",
    "\n",
    "# Reproducibility\n",
    "# https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "SEED = 42\n",
    "L.seed_everything(SEED, workers=True)\n",
    "\n",
    "print(\n",
    "    f'torch: {torch.__version__}, cuda: {torch.cuda.is_available()}, lightning: {L.__version__}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'resnet18', 'resnet34', 'resnet50', 'resnet101', ...\n",
    "ENCODER = 'resnet18'\n",
    "DECODER = 'unet'  # 'unet', 'unetplusplus', ...\n",
    "FREEZE_ENCODER = True\n",
    "MAX_EPOCHS = 500\n",
    "MONITOR = 'val_loss'\n",
    "PATIENCE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = Path('logs')\n",
    "EXPERIMENT_NAME = f'{DECODER}-{ENCODER}'\n",
    "VERSION = 0\n",
    "EXPERIMENT_DIR = LOG_DIR / EXPERIMENT_NAME / f'version_{VERSION}'\n",
    "DATASET_URL = 'https://doi.org/10.5281/zenodo.10732179'\n",
    "\n",
    "METRICS_CSV_NAME = 'metrics.csv'\n",
    "LEARNING_CURVES_PNG_NAME = 'learning_curves.png'\n",
    "PREDICTIONS_PNG_NAME = 'predictions.png'\n",
    "TRAIN_TRANSFORMS_JSON_NAME = 'train_transforms.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "DATASET_DIR = Path('data')  # 'Corrosion_in_Industrial_Complexes_in_Ostrava'\n",
    "\n",
    "TRAIN_SET_DIR = 'train'\n",
    "VALID_SET_DIR = 'validation'\n",
    "TEST_SET_DIR = 'test'\n",
    "\n",
    "IMAGES_DIR = 'images'\n",
    "MASKS_DIR = 'masks'\n",
    "\n",
    "TRAIN_IMAGES = sorted((DATASET_DIR / TRAIN_SET_DIR / IMAGES_DIR).glob('*.jpg'))\n",
    "TRAIN_MASKS = sorted((DATASET_DIR / TRAIN_SET_DIR / MASKS_DIR).glob('*.png'))\n",
    "\n",
    "VALID_IMAGES = sorted((DATASET_DIR / VALID_SET_DIR / IMAGES_DIR).glob('*.jpg'))\n",
    "VALID_MASKS = sorted((DATASET_DIR / VALID_SET_DIR / MASKS_DIR).glob('*.png'))\n",
    "\n",
    "TEST_IMAGES = sorted((DATASET_DIR / TEST_SET_DIR / IMAGES_DIR).glob('*.jpg'))\n",
    "TEST_MASKS = sorted((DATASET_DIR / TEST_SET_DIR / MASKS_DIR).glob('*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random(TRAIN_IMAGES, TRAIN_MASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random(VALID_IMAGES, VALID_MASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random(TEST_IMAGES, TEST_MASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = compose_transforms(\n",
    "    [\n",
    "        resize_transforms(image_size=IMAGE_SIZE),\n",
    "        hard_transforms(),\n",
    "        post_transforms(),\n",
    "    ]\n",
    ")\n",
    "valid_transforms = compose_transforms(\n",
    "    [\n",
    "        pre_transforms(image_size=IMAGE_SIZE),\n",
    "        post_transforms(),\n",
    "    ]\n",
    ")\n",
    "test_transforms = compose_transforms(\n",
    "    [\n",
    "        pre_transforms(image_size=IMAGE_SIZE),\n",
    "        post_transforms(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "show_transforms = compose_transforms([resize_transforms(), hard_transforms()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random(TRAIN_IMAGES, TRAIN_MASKS, transforms=show_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transforms.transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = SegmentationDatasetLoader(\n",
    "    train=SegmentationDatasetSplit(images=TRAIN_IMAGES, masks=TRAIN_MASKS),\n",
    "    valid=SegmentationDatasetSplit(images=VALID_IMAGES, masks=VALID_MASKS),\n",
    "    test=SegmentationDatasetSplit(images=TEST_IMAGES, masks=TEST_MASKS),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = dataset_loader.get_loaders(\n",
    "    # set to zero if RuntimeError: Trying to resize storage that is not resizable\n",
    "    num_workers=int(os.cpu_count()),\n",
    "    batch_size={'train': BATCH_SIZE, 'valid': 1, 'test': 1},\n",
    "    train_transforms=train_transforms,\n",
    "    valid_transforms=valid_transforms,\n",
    "    test_transforms=test_transforms,\n",
    ")\n",
    "\n",
    "train_dataloader = loaders['train']\n",
    "valid_dataloader = loaders['valid']\n",
    "test_dataloader = loaders['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CorrosionModel(\n",
    "    DECODER, ENCODER, in_channels=3, out_classes=1, freeze_encoder=FREEZE_ENCODER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_logger = TensorBoardLogger(LOG_DIR, name=EXPERIMENT_NAME, version=VERSION)\n",
    "csv_logger = CSVLogger(LOG_DIR, name=EXPERIMENT_NAME, version=VERSION)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=MONITOR,\n",
    "    mode='min',\n",
    "    patience=PATIENCE,\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=EXPERIMENT_DIR,\n",
    "    filename='{epoch}-{val_loss:3f}',\n",
    "    monitor=MONITOR,\n",
    "    save_top_k=1,  # save only the best model\n",
    "    mode='min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    logger=[tb_logger, csv_logger],\n",
    "    callbacks=[model_checkpoint, early_stopping],\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    log_every_n_steps=1,  # log every batch\n",
    "    # https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "    deterministic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = list(EXPERIMENT_DIR.glob('*.ckpt'))[0]\n",
    "model_ = CorrosionModel.load_from_checkpoint(model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = trainer.validate(model_, dataloaders=valid_dataloader, verbose=False)[0]\n",
    "pprint(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = trainer.test(model_, dataloaders=test_dataloader, verbose=False)[0]\n",
    "pprint(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_compact(\n",
    "    model_, test_dataloader, save_path=EXPERIMENT_DIR / PREDICTIONS_PNG_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_predictions(model_, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(\n",
    "    EXPERIMENT_DIR / METRICS_CSV_NAME,\n",
    "    save_path=EXPERIMENT_DIR / LEARNING_CURVES_PNG_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transforms for experiment logging\n",
    "albu.save(train_transforms, EXPERIMENT_DIR / TRAIN_TRANSFORMS_JSON_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_early_stopping_epoch() -> int | None:\n",
    "    checkpoint = list(EXPERIMENT_DIR.glob('*.ckpt'))[0].stem\n",
    "    pattern = r'epoch=(\\d+)'\n",
    "    match = re.search(pattern, checkpoint)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dict_to_mlflow(dictionary: dict[str, float]) -> None:\n",
    "    for k, v in dictionary.items():\n",
    "        mlflow.log_metric(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f'{EXPERIMENT_NAME}-v{VERSION}') as run:\n",
    "    try:\n",
    "        mlflow.set_tag('Branch', get_current_branch())\n",
    "        mlflow.set_tag('Commit ID', get_commit_id())\n",
    "        mlflow.set_tag('Dataset', DATASET_URL)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    log_dict_to_mlflow(dict(valid_metrics))\n",
    "    log_dict_to_mlflow(dict(test_metrics))\n",
    "\n",
    "    mlflow.log_param('encoder', ENCODER)\n",
    "    mlflow.log_param('decoder', DECODER)\n",
    "    mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "    mlflow.log_param('max_epochs', trainer.max_epochs)\n",
    "    mlflow.log_param('early_stopping', get_early_stopping_epoch())\n",
    "    mlflow.log_param('monitor', MONITOR)\n",
    "    mlflow.log_param('patience', PATIENCE)\n",
    "    mlflow.log_param('image_size', IMAGE_SIZE)\n",
    "    mlflow.log_param('frozen_encoder', FREEZE_ENCODER)\n",
    "\n",
    "    # Models are versioned by default\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model_,\n",
    "        artifact_path='model',\n",
    "        registered_model_name=f'pytorch-{DECODER}-{ENCODER}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a boto3.client object\n",
    "s3 = get_repo_bucket_client(f'{USER_NAME}/{REPO_NAME}')\n",
    "\n",
    "# Upload files to the bucket\n",
    "s3.upload_file(\n",
    "    Bucket=REPO_NAME,\n",
    "    Filename=str(EXPERIMENT_DIR / METRICS_CSV_NAME),\n",
    "    Key=str(EXPERIMENT_DIR / METRICS_CSV_NAME),\n",
    ")\n",
    "s3.upload_file(\n",
    "    Bucket=REPO_NAME,\n",
    "    Filename=str(EXPERIMENT_DIR / PREDICTIONS_PNG_NAME),\n",
    "    Key=str(EXPERIMENT_DIR / PREDICTIONS_PNG_NAME),\n",
    ")\n",
    "s3.upload_file(\n",
    "    Bucket=REPO_NAME,\n",
    "    Filename=str(EXPERIMENT_DIR / LEARNING_CURVES_PNG_NAME),\n",
    "    Key=str(EXPERIMENT_DIR / LEARNING_CURVES_PNG_NAME),\n",
    ")\n",
    "s3.upload_file(\n",
    "    Bucket=REPO_NAME,\n",
    "    Filename=str(EXPERIMENT_DIR / TRAIN_TRANSFORMS_JSON_NAME),\n",
    "    Key=str(EXPERIMENT_DIR / TRAIN_TRANSFORMS_JSON_NAME),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
